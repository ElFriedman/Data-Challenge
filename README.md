# Data-Challenge
Dartmouth Dali Lab Data Challenge Submission



After showing my ability to work with Big Data with the WIID data set, I thought demonstrating my ability to make good use of “tiny data” would better show my ability to effectively manipulate datasets rather than an ML model. While large datasets can be extremely useful in informing decision making, there are many times where we don’t have it is simply unfeasible or impossible to gather this much information, but instead we only have a small sample of data, what Karl Broman calls Tiny Data. In cases such as these, we can make use of a method known as Approximate Bayesian Computation (ABC). ABC relies upon being able to create a generative model that produces the same type of data you want to model. Once this is made, we sample from a prior distribution of parameters to simulate a dataset, and if it matches the actual data, you store the parameter values. This process is repeated a large amount of times until we are left with a posterior distribution, which represents the probability that each parameter set is the true prior. 

For my model, I created a dataset based off of a situation that I was in recently. Your friend, lets call him Keggy, claims that he has figured out the trick behind Rock Paper Scissors. You obviously don’t believe him and decide to play 20 games. He manages to win 10 of the 20 games, which is significantly better than the average of 6.7 that one would expect. We could analyze this data by assuming a success rate of .333 and calculate the cumulative probability of winning 10 or more games on a binomial distribution, which works out to a 9.0% chance and thus not significant. However, this does not account for our prior knowledge of the problem, as well as the fact that for more complex simulations this is not possible to compute. 

Instead, we could use Approximate Bayesian Computation to determine the posterior distribution of parameters (in this case, Keggy’s chance of winning), which I coded in R. To begin, I used a beta distribution (alpha = 31, beta = 60, see Figure 5) to pull the parameter from. For each simulation, I then generated 20 random numbers from 0 to 1, and for each number less than the parameter of winning chance, I counted it as a victory. The result of this can be seen in Figure 6. I then used exact rejection, meaning I only stored the parameter values if the simulation resulted in exactly 10 wins of the 20, adding these to a distribution which is shown in Figure 7. The mean value of this posterior distribution was 0.37, indicating that perhaps Keggy had an edge in Rock Paper Scissors. 
